= 3. Theoretical Part

  A comparative analysis of machine learning algorithms for binary intrusion detection on the NSL-KDD dataset reveals a complex and varied performance landscape, where no single algorithmic class demonstrates absolute superiority. As mentioned in Chapter 2, the dataset eliminated redundant records that might bias classifiers toward common patterns. The NSL-KDD dataset was created as a refined benchmark to address the shortcomings of the previous KDD'99 dataset #cite(<sapre2019robust>); #cite(<divekar2018benchmarking>). The effectiveness of classical machine learning models, including Decision Trees, K-Nearest Neighbors, Support Vector Machines, Random Forests, and Logistic Regression, against basic Artificial Neural Networks for the binary task of differentiating "Normal" network traffic from "Attack" traffic is compared in this review by combining results from several studies. Although results are very sensitive to experimental methodology, feature selection, and hyperparameter tuning, the evidence indicates that while classical algorithms can achieve excellent performance, a well-configured Artificial Neural Network frequently matches or surpasses them.

  Intrusion detection systems typically rely on either anomaly detection or signature matching to distinguish between benign and malicious network activity. The NSL-KDD benchmark dataset is widely used in research because it removes the significant redundancy and some distribution shifts found in the original KDD'99 dataset, while still preserving the same 41 features and attack taxonomy #cite(<divekar2018benchmarking>).
  Even if the dataset has some advantages over its predecessor, preprocessing remains important. The official binary classification training set contains 67,343 normal and 58,630 intrusion records, resulting in a more balanced class ratio compared to the multi-class version. Before training, most studies apply one-hot encoding to categorical features and normalize numerical variables using either L2 normalization or z-score standardization #cite(<sapre2019robust>), #cite(<ravipati2019intrusion>), #cite(<ingre2015performance>).


 The algorithms reviewed in this work adopt a different approach to traffic classification. Decision Trees (DT) and their ensemble variant, Random Forests (RF), build rule-based models to distinguish between classes. By aggregating the predictions of multiple trees, RFs generally achieve higher accuracy and greater robustness against overfitting #cite(<pai2021comparative>). Support Vector Machines (SVM), a potentially effective but computationally demanding technique, seek to identify the best separating hyperplane between classes in a high-dimensional feature space #cite(<sapre2019robust>). On the other hand, complex, non-linear relationships in the data are learned by Artificial Neural Networks (ANN), which are made up of interconnected layers of nodes. Their ability to model extremely complex patterns is highly dependent on their architecture, including the number of layers and neurons as well as the optimization process #cite(<mohammed2020multilayer>), #cite(<choras2021intrusion>).

  Significant variation is revealed by a direct comparison of binary classification performance across the literature that is provided. According to certain research, classical machine algorithms perform better. For example, Random Forest has an impressive 99.7% accuracy on NSL-KDD, according to a review by #cite(<ravipati2019intrusion>). Likewise, they discovered that an SVM outperformed their ANN (0.7557) and Random Forest (0.7528) implementations, achieving the highest F1-score (0.8476) in their comparison. Although they also warn that their SVM and RF models were trained on a small data sample due to computational expense (SVM at 0.05\%\) and random forest at \(0.1\%\) of the training set, they specifically mentioned that their ANN and RF models suffered from low recall, indicating a difficulty in correctly identifying attack instances) #cite(<sapre2019robust>).

  However, other studies show that ANNs have a lot of potenntial. #cite(<ingre2015performance>) set a baseline for ANN performance, reducing the feature set to 29 attributes and achieving an accuracy of 81.2% for binary classification. The findings of #cite(<divekar2018benchmarking>) show similar weighted F1-scores of about 81–82% for ANN, SVM, and Random Forest on the NSL-KDD binary task. These results temper claims of superiority and suggest performance parity under the specific experimental conditions.

  The absence of standardized evaluation procedures is primarily to blame for these disparities in reported results. Studies differ in the number of input features, ranging from 20 #cite(<divekar2018benchmarking>) to 38 #cite(<mohammed2020multilayer>). The comparability of results is also impacted by the different data splitting techniques used by different researchers. For example, some use the standard KDDTrain+/KDDTest+ files #cite(<sapre2019robust>), while others use custom 70/30 splits #cite(<mohammed2020multilayer>).

  In conclusion, there is some support but not enough evidence to conclusively prove that classical machine learning algorithms can perform on par with or better than a basic ANN for binary intrusion detection on the NSL-KDD dataset. According to the literature, models such as Random Forest and SVM can perform better than some ANN implementations, particularly when the ANN is not set up optimally or when evaluation metrics like F1-score are given priority #cite(<ravipati2019intrusion>), #cite(<sapre2019robust>).  In the end the performance depends less on the type of algorithm you choose and more on how you implement it. Things like feature engineering, data handling, and careful tuning of hyperparameters make the biggest difference. While classical models may offer a robust and less complex baseline, ANNs retain the potential for higher performance, though this comes at the cost of increased complexity and tuning effort. For a fair and reproducible comparison, a new study should therefore include linear SVM, random forest, decision tree, logistic regression, k‑nearest neighbors, and a shallow MLP. It should also apply consistent one‑hot encoding and normalization, perform hyperparameter selection. Under these conditions, the hypothesis that classical machine learning algorithms can match or outperform a simple ANN on binary NSL‑KDD while being lighter‑weight is well supported by the available evidence.